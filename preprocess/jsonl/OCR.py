# import os
# import json
# import easyocr
#
# # ========== é…ç½® ==========
# dataset_dir = "data"
# frames_root_dir = "data/frames_16"  # è§†é¢‘å¸§æ ¹ç›®å½•
# output_jsonl = "data/OCR.jsonl"  # è¾“å‡ºæ–‡ä»¶
#
# # ========== åˆå§‹åŒ–OCR ==========
# reader = easyocr.Reader(['en'], gpu=True)  # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
#
#
# # ========== å¤„ç†å‡½æ•° ==========
# def extract_ocr_from_video(video_folder):
#     """ä»è§†é¢‘æ–‡ä»¶å¤¹æå–OCRæ–‡æœ¬"""
#     ocr_texts = []
#
#     # éå†è§†é¢‘æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å¸§æ–‡ä»¶
#     for file in os.listdir(video_folder):
#         if file.startswith('frame_') and file.endswith(('.jpg', '.png')):
#             frame_path = os.path.join(video_folder, file)
#
#             try:
#                 # æå–æ–‡æœ¬
#                 results = reader.readtext(frame_path, detail=0, paragraph=True)
#                 for text in results:
#                     if text.strip():
#                         ocr_texts.append(text.strip())
#             except:
#                 continue
#
#     # å»é‡åˆå¹¶
#     unique_texts = []
#     seen = set()
#     for text in ocr_texts:
#         if text not in seen:
#             seen.add(text)
#             unique_texts.append(text)
#
#     return " ".join(unique_texts)
#
#
# # ========== ä¸»ç¨‹åº ==========
# print("å¼€å§‹æå–OCRæ–‡æœ¬...")
#
# with open(output_jsonl, 'w', encoding='utf-8') as f_out:
#     # éå†framesæ ¹ç›®å½•ä¸‹çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶å¤¹
#     for video_name in os.listdir(frames_root_dir):
#         video_path = os.path.join(frames_root_dir, video_name)
#
#         if os.path.isdir(video_path):  # ç¡®ä¿æ˜¯æ–‡ä»¶å¤¹
#             print(f"å¤„ç†: {video_name}")
#
#             # æå–OCRæ–‡æœ¬
#             ocr_text = extract_ocr_from_video(video_path)
#
#             # å†™å…¥JSONL
#             json_data = {"vid": video_name, "ocr": ocr_text}
#             f_out.write(json.dumps(json_data, ensure_ascii=False) + "\n")
#
# print(f"å®Œæˆï¼ç»“æœä¿å­˜è‡³: {output_jsonl}")

# import json
# import easyocr
#
# # ========== é…ç½® ==========
# dataset_dir = r"D:\code\LAB\MoRE2026\data"
# frames_root_dir = r"D:\code\LAB\MoRE2026\data\frames_32"  # è§†é¢‘å¸§æ ¹ç›®å½•
# output_jsonl = r"D:\code\LAB\MoRE2026\data\ocr.jsonl"  # è¾“å‡ºæ–‡ä»¶
#
# # ========== åˆå§‹åŒ–OCR ==========
# reader = easyocr.Reader(['en'], gpu=True)  # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
#
#
# # ========== å¤„ç†å‡½æ•° ==========
# def extract_ocr_from_video(video_folder):
#     """ä»è§†é¢‘æ–‡ä»¶å¤¹æå–OCRæ–‡æœ¬"""
#     ocr_texts = []
#
#     # éå†è§†é¢‘æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å¸§æ–‡ä»¶
#     for file in os.listdir(video_folder):
#         if file.startswith('frame_') and file.endswith(('.jpg', '.png')):
#             frame_path = os.path.join(video_folder, file)
#
#             try:
#                 # æå–æ–‡æœ¬
#                 results = reader.readtext(frame_path, detail=0, paragraph=True)
#                 for text in results:
#                     if text.strip():
#                         ocr_texts.append(text.strip())
#             except:
#                 continue
#
#     # å»é‡åˆå¹¶
#     unique_texts = []
#     seen = set()
#     for text in ocr_texts:
#         if text not in seen:
#             seen.add(text)
#             unique_texts.append(text)
#
#     return " ".join(unique_texts)
#
#
# # ========== ä¸»ç¨‹åº ==========
# print("å¼€å§‹æå–OCRæ–‡æœ¬...")
#
# with open(output_jsonl, 'w', encoding='utf-8') as f_out:
#     # éå†framesæ ¹ç›®å½•ä¸‹çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶å¤¹
#     for video_name in os.listdir(frames_root_dir):
#         video_path = os.path.join(frames_root_dir, video_name)
#
#         if os.path.isdir(video_path):  # ç¡®ä¿æ˜¯æ–‡ä»¶å¤¹
#             print(f"å¤„ç†: {video_name}")
#
#             # æå–OCRæ–‡æœ¬
#             ocr_text = extract_ocr_from_video(video_path)
#
#             # å†™å…¥JSONL
#             json_data = {"vid": video_name, "ocr": ocr_text}
#             f_out.write(json.dumps(json_data, ensure_ascii=False) + "\n")
#
# print(f"å®Œæˆï¼ç»“æœä¿å­˜è‡³: {output_jsonl}")

import os
import json
import numpy as np
from decord import VideoReader, cpu
import easyocr
from tqdm import tqdm
import torch

# ========== é…ç½® ==========
video_dir = "data/videos"
output_jsonl = "data/OCR.jsonl"

# ========== æœ¬åœ°æ¨¡å‹è·¯å¾„é…ç½® ==========
# è¯·ä¿®æ”¹ä¸ºä½ æœ¬åœ°çš„ EasyOCR æ¨¡å‹è·¯å¾„
local_model_path = "/root/autodl-tmp/MoRE/MoRE2026-Cloud/models/easyocr_models"  # ä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„

# æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
if not os.path.exists(local_model_path):
    print(f"âš  è­¦å‘Š: æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {local_model_path}")
    print("è¯·ä¿®æ”¹ local_model_path ä¸ºæ­£ç¡®çš„è·¯å¾„")
    # å°è¯•åˆ›å»ºç›®å½•
    os.makedirs(local_model_path, exist_ok=True)
    print(f"å·²åˆ›å»ºç›®å½•: {local_model_path}")

# æ£€æŸ¥ CUDA å¯ç”¨æ€§
cuda_available = torch.cuda.is_available()
print("=" * 60)
print("ğŸ¬ è§†é¢‘ OCR æ–‡æœ¬æå– (ä½¿ç”¨æœ¬åœ°æ¨¡å‹)")
print("=" * 60)
print(f"æœ¬åœ°æ¨¡å‹è·¯å¾„: {local_model_path}")
print(f"PyTorch CUDA å¯ç”¨: {cuda_available}")
print("=" * 60)

# åˆå§‹åŒ–EasyOCRï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰
try:
    # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œç¦æ­¢ä¸‹è½½
    reader = easyocr.Reader(
        lang_list=['en'],  # åªä½¿ç”¨è‹±è¯­
        gpu=cuda_available,  # æ ¹æ®CUDAå¯ç”¨æ€§å†³å®š
        model_storage_directory=local_model_path,
        download_enabled=False,  # ç¦æ­¢ä¸‹è½½
        detector=True,
        recognizer=True,
        verbose=False  # å‡å°‘è¾“å‡º
    )
    print(f"âœ… EasyOCR åˆå§‹åŒ–æˆåŠŸ ({'GPU' if cuda_available else 'CPU'}æ¨¡å¼ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹)")
except Exception as e:
    print(f"âŒ EasyOCR åˆå§‹åŒ–å¤±è´¥: {e}")
    print("å°è¯•ç®€åŒ–åˆå§‹åŒ–...")
    try:
        reader = easyocr.Reader(['en'], gpu=cuda_available)
        print("âœ… EasyOCR ç®€åŒ–åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e2:
        print(f"âŒ EasyOCR å®Œå…¨å¤±è´¥: {e2}")
        raise

print("=" * 60)


def extract_ocr_from_video(video_path, target_frames=16):
    """æå–è§†é¢‘ä¸­çš„OCRæ–‡æœ¬"""
    try:
        video_name = os.path.basename(video_path)
        print(f"ğŸ“¹ å¤„ç†: {video_name}")

        # å§‹ç»ˆä½¿ç”¨ CPU è¯»å–è§†é¢‘ï¼ˆæ›´ç¨³å®šï¼‰
        ctx = cpu(0)

        # è¯»å–è§†é¢‘
        vr = VideoReader(video_path, ctx=ctx)
        total_frames = len(vr)

        # ä¼˜åŒ–é‡‡æ ·ç­–ç•¥ï¼šå‡å°‘é‡‡æ ·å¸§æ•°ä»¥æé«˜é€Ÿåº¦
        if total_frames < 10:  # è§†é¢‘å¤ªçŸ­
            target_frames = min(total_frames, 4)
        elif total_frames < 30:
            target_frames = 8
        else:
            target_frames = 12  # å‡å°‘é‡‡æ ·å¸§æ•°

        # é€‰æ‹©é‡‡æ ·å¸§
        if total_frames < target_frames:
            indices = list(range(total_frames))
        else:
            # å‡åŒ€é‡‡æ ·
            indices = np.linspace(0, total_frames - 1, target_frames, dtype=int)

        print(f"  æ€»å¸§æ•°: {total_frames}, é‡‡æ ·: {len(indices)}å¸§")

        ocr_texts = []
        processed_frames = 0

        # é€å¸§å¤„ç†
        for idx in indices:
            try:
                # è¯»å–å¸§
                frame = vr[idx].asnumpy()
                # BGR to RGB
                frame_rgb = frame[:, :, ::-1]

                # OCRè¯†åˆ«
                results = reader.readtext(
                    frame_rgb,
                    detail=0,  # åªè¿”å›æ–‡æœ¬
                    paragraph=True,  # æ®µè½æ¨¡å¼
                    width_ths=0.7,
                    text_threshold=0.3,
                    batch_size=4  # æ‰¹é‡å¤„ç†
                )

                # å¤„ç†ç»“æœ
                for text in results:
                    text_clean = text.strip()
                    if text_clean and len(text_clean) > 2:
                        ocr_texts.append(text_clean)

                processed_frames += 1

            except Exception as frame_error:
                print(f"  å¸§ {idx} å¤„ç†å¤±è´¥: {str(frame_error)[:50]}")
                continue

        print(f"  æˆåŠŸå¤„ç†: {processed_frames}/{len(indices)} å¸§")

        # å»é‡å’Œåˆå¹¶
        if not ocr_texts:
            print(f"  æœªè¯†åˆ«åˆ°æ–‡æœ¬")
            return ""

        # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
        unique_texts = []
        seen = set()
        for text in ocr_texts:
            if text not in seen:
                seen.add(text)
                unique_texts.append(text)

        print(f"  è¯†åˆ«åˆ° {len(unique_texts)} æ¡æ–‡æœ¬")

        # åˆå¹¶æ–‡æœ¬ï¼Œé™åˆ¶é•¿åº¦
        result = " ".join(unique_texts[:50])  # é™åˆ¶æœ€å¤§æ–‡æœ¬æ•°é‡
        if len(result) > 500:  # é™åˆ¶å­—ç¬¦æ•°
            result = result[:500] + "..."

        return result

    except Exception as e:
        print(f"âŒ å¤„ç†è§†é¢‘ {os.path.basename(video_path)} æ—¶å‡ºé”™: {str(e)[:100]}")
        return ""


# ========== ä¸»ç¨‹åº ==========
def main():
    print(f"è§†é¢‘ç›®å½•: {video_dir}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_jsonl}")
    print("=" * 60)

    # æ”¯æŒçš„è§†é¢‘æ ¼å¼
    video_extensions = ('.mp4')

    # æ”¶é›†è§†é¢‘æ–‡ä»¶
    video_files = []
    for f in os.listdir(video_dir):
        if f.lower().endswith(video_extensions):
            video_files.append(f)

    if not video_files:
        print(f"âŒ é”™è¯¯: åœ¨ {video_dir} ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        return

    print(f"âœ… æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_jsonl), exist_ok=True)

    # å¤„ç†è§†é¢‘
    processed_count = 0
    failed_count = 0

    # è¿›åº¦æ¡
    pbar = tqdm(video_files, desc="æ•´ä½“è¿›åº¦", unit="è§†é¢‘")

    with open(output_jsonl, 'w', encoding='utf-8') as f_out:
        for video_file in pbar:
            video_path = os.path.join(video_dir, video_file)
            video_name = os.path.splitext(video_file)[0]

            # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.set_description(f"å¤„ç† {video_name[:20]}...")

            try:
                # æå–OCRæ–‡æœ¬
                ocr_text = extract_ocr_from_video(video_path)

                # å†™å…¥JSONL
                json_data = {"vid": video_name, "ocr": ocr_text}
                f_out.write(json.dumps(json_data, ensure_ascii=False) + "\n")
                f_out.flush()

                processed_count += 1

                # æ˜¾ç¤ºå¤„ç†ç»“æœé¢„è§ˆ
                if ocr_text:
                    pbar.set_postfix({
                        "çŠ¶æ€": "æˆåŠŸ",
                        "æ–‡æœ¬é•¿åº¦": len(ocr_text),
                        "è¿›åº¦": f"{processed_count}/{len(video_files)}"
                    })
                else:
                    pbar.set_postfix({
                        "çŠ¶æ€": "æ— æ–‡æœ¬",
                        "è¿›åº¦": f"{processed_count}/{len(video_files)}"
                    })

            except Exception as e:
                pbar.set_postfix({"çŠ¶æ€": "å¤±è´¥"})
                print(f"\nâš  å¤„ç† {video_file} å¤±è´¥: {str(e)[:100]}")
                # å†™å…¥ç©ºè®°å½•
                json_data = {"vid": video_name, "ocr": ""}
                f_out.write(json.dumps(json_data, ensure_ascii=False) + "\n")
                failed_count += 1

    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"âœ… æˆåŠŸ: {processed_count} ä¸ª")
    print(f"âŒ å¤±è´¥: {failed_count} ä¸ª")
    print(f"ğŸ“Š æ€»è®¡: {processed_count + failed_count} ä¸ª")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_jsonl}")

    # æ˜¾ç¤ºç¤ºä¾‹
    print("\nğŸ“‹ å‰3æ¡è®°å½•ç¤ºä¾‹:")
    try:
        with open(output_jsonl, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            for i, line in enumerate(lines[:3]):
                data = json.loads(line.strip())
                ocr_preview = data['ocr'][:80] + "..." if len(data['ocr']) > 80 else data['ocr']
                print(f"  {i + 1}. {data['vid']}: {ocr_preview}")
    except Exception as e:
        print(f"è¯»å–ç¤ºä¾‹å¤±è´¥: {e}")


if __name__ == "__main__":
    main()


# # åŠ å…¥æ¨¡ç³ŠåŒ¹é…æœºåˆ¶ï¼Œè¯å…¸
#
# import os
# import json
# import torch
# import pandas as pd
# from tqdm import tqdm
# from torch.utils.data import Dataset, DataLoader
# from transformers import AutoModel, AutoTokenizer
#
# # ---------- Paths & model ----------
# dataset_dir = 'data'
# output_file = os.path.join(dataset_dir, 'fea/fea_transcript_bert-base-uncased.pt')
# model_id = "/root/autodl-tmp/MoRE/MoRE2026-Cloud/models/bert/bert-base-uncased"
#
# model = AutoModel.from_pretrained(model_id, device_map='cuda')
# processor = AutoTokenizer.from_pretrained(model_id)
#
# # ---------- Sensitive words (one per line, case-sensitive) ----------
# sensitive_file = os.path.join(dataset_dir, 'sensitive_words.txt')
# with open(sensitive_file, 'r', encoding='utf-8') as f:
#     sensitive_words = [line.strip() for line in f if line.strip()]
#
# # ---------- Levenshtein distance & fuzzy match ----------
# def levenshtein(a: str, b: str) -> int:
#     la, lb = len(a), len(b)
#     if la == 0: return lb
#     if lb == 0: return la
#     prev_row = list(range(lb + 1))
#     for i in range(1, la + 1):
#         cur_row = [i] + [0] * lb
#         for j in range(1, lb + 1):
#             cost = 0 if a[i-1] == b[j-1] else 1
#             cur_row[j] = min(prev_row[j] + 1, cur_row[j-1] + 1, prev_row[j-1] + cost)
#         prev_row = cur_row
#     return prev_row[lb]
#
# def fuzzy_in_text(sensitive: str, text: str, max_dist: int = 1) -> bool:
#     if not sensitive or not text:
#         return False
#     ls = len(sensitive)
#     min_l = max(1, ls - 1)
#     max_l = ls + 1
#     n = len(text)
#     for L in range(min_l, max_l + 1):
#         if L > n:
#             continue
#         for i in range(0, n - L + 1):
#             sub = text[i:i+L]
#             if levenshtein(sensitive, sub) <= max_dist:
#                 return True
#     return False
#
# # ---------- OCR processing: produce jsonl with {"vid":..., "ocr": "" or "word1 word2"} ----------
# ocr_file = os.path.join(dataset_dir, 'ocr.jsonl')
# ocr_out_file = os.path.join(dataset_dir, 'ocr_sensitive.jsonl')
#
# if os.path.exists(ocr_file):
#     out_lines = []
#     with open(ocr_file, 'r', encoding='utf-8') as f:
#         for line in f:
#             line = line.strip()
#             if not line:
#                 continue
#             try:
#                 obj = json.loads(line)
#             except Exception:
#                 continue
#             vid = obj.get('vid')
#             # adjust these extractions if your ocr.jsonl structure differs
#             text_field = obj.get('ocr') if 'ocr' in obj else obj.get('text', '')
#             if isinstance(text_field, list):
#                 text = ' '.join(str(x) for x in text_field)
#             elif isinstance(text_field, dict):
#                 text = ' '.join(str(v) for v in text_field.values())
#             else:
#                 text = str(text_field)
#
#             matched = []
#             for sw in sensitive_words:
#                 try:
#                     if fuzzy_in_text(sw, text, max_dist=1):
#                         matched.append(sw)
#                 except Exception:
#                     continue
#             ocr_val = "" if not matched else " ".join(matched)
#             out_lines.append({"vid": vid, "ocr": ocr_val})
#
#     with open(ocr_out_file, 'w', encoding='utf-8') as fout:
#         for rec in out_lines:
#             fout.write(json.dumps(rec, ensure_ascii=False) + '\n')
# else:
#     # write nothing or create empty file with vids if desired
#     print(f"OCR file not found at {ocr_file}, skipping OCR-sensitive generation.")
#
# # ---------- Feature extraction for transcripts + captions (similar to your original) ----------
# class MyDataset(Dataset):
#     def __init__(self):
#         vid_file = "data/vids/vids.csv"
#         with open(vid_file, 'r', encoding='utf-8') as f:
#             self.vids = [line.strip() for line in f if line.strip()]
#         self.trans_df = pd.read_json(os.path.join(dataset_dir, 'speech.jsonl'), lines=True)
#         self.caption_df = pd.read_json(os.path.join(dataset_dir, 'caption.jsonl'), lines=True)
#
#     def __len__(self):
#         return len(self.vids)
#
#     def __getitem__(self, index):
#         vid = self.vids[index]
#         trans = ''
#         try:
#             trans_row = self.trans_df[self.trans_df['vid'] == vid]
#             if len(trans_row) > 0:
#                 trans = trans_row['transcript'].values[0]
#                 if isinstance(trans, dict) and 'transcript' in trans:
#                     trans = trans['transcript']
#                 if pd.isna(trans) or (isinstance(trans, str) and trans.strip() == ''):
#                     trans = ''
#         except Exception:
#             trans = ''
#
#         caption = ''
#         try:
#             cap_row = self.caption_df[self.caption_df['vid'] == vid]
#             if len(cap_row) > 0:
#                 caption = cap_row['text'].values[0]
#                 if isinstance(caption, dict) and 'text' in caption:
#                     caption = caption['text']
#                 if pd.isna(caption) or (isinstance(caption, str) and caption.strip() == ''):
#                     caption = ''
#         except Exception:
#             caption = ''
#
#         text = f'{caption}\n{trans}'
#         return vid, text
#
# def customed_collate_fn(batch):
#     vids, texts = zip(*batch)
#     inputs = processor(list(texts), padding='max_length', truncation=True, return_tensors='pt', max_length=512)
#     return vids, inputs
#
# save_dict = {}
# dataloader = DataLoader(MyDataset(), batch_size=1, collate_fn=customed_collate_fn, num_workers=0, shuffle=True)
#
# model.eval()
# for batch in tqdm(dataloader):
#     with torch.no_grad():
#         vids, inputs = batch
#         inputs = {k: v.to('cuda') for k, v in inputs.items()}
#         outputs = model(**inputs)
#         last_hidden = outputs['last_hidden_state'][:, 0, :]
#         pooler_output = last_hidden.detach().cpu()
#         for i, vid in enumerate(vids):
#             save_dict[vid] = pooler_output[i]
#
# torch.save(save_dict, output_file)
# print(f"Saved features to {output_file}")
